# Qwen2.5-VL-3B Full Fine-tuning (No LoRA)
#
# This config demonstrates full fine-tuning without LoRA.
# Requires more GPU memory but may achieve better performance.

experiment_name: "qwen25_vl_full_finetune"

model:
  name: "Qwen/Qwen2.5-VL-3B-Instruct"
  device: "cuda"
  torch_dtype: "float16"

  # Disable LoRA for full fine-tuning
  use_lora: false

  # Generation parameters
  max_new_tokens: 128
  temperature: 0.7
  do_sample: true

  # Training options
  freeze_vision_encoder: false

trainer:
  batch_size: 2  # Smaller batch for full fine-tuning (memory)
  learning_rate: null  # Auto-detect: will use 1e-5 for full fine-tuning
  num_training_steps: 100
  save_every: 10
  queue_timeout: 5.0

  algorithm: "rejection_sampling"
  reward_threshold: 0.0

actor:
  max_steps_per_episode: 50
  action_format: "json"
  screenshot_size: [224, 224]
  task_prompt: "Complete the data entry task"
  session_type: "simple_data_entry"
  action_delay: 1.0
  data_dir: null

actor_pool:
  target_concurrent_actors: 2
  max_concurrent_per_vm: 2
  monitor_interval: 2.0

environment:
  vm_urls:
    - "http://localhost:8000"
  timeout: 30
  max_retries: 3

logging:
  log_level: "INFO"
  log_dir: "experiments/qwen25_vl_full_finetune/logs"
  checkpoint_dir: "experiments/qwen25_vl_full_finetune/checkpoints"
  trajectory_dir: null
  verbose: false
